üåç Iniziando la simulazione per il task: Come pu√≤ un sistema di AI migliorare progressivamente la sua capacit√† di rispondere a domande complesse?
Limite cicli: 9

Traceback (most recent call last):
  File "/app/src/main_graph.py", line 104, in <module>
    for event in _graph.stream(_initial_state, config=config):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/langgraph/pregel/main.py", line 2646, in stream
    for _ in runner.tick(
             ^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/langgraph/pregel/_runner.py", line 167, in tick
    run_with_retry(
  File "/usr/local/lib/python3.12/site-packages/langgraph/pregel/_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/src/nodes/explore.py", line 27, in explore
    response = llm.invoke([HumanMessage(content=prompt)])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 402, in invoke
    self.generate_prompt(
  File "/usr/local/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1123, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 933, in generate
    self._generate_with_cache(
  File "/usr/local/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1235, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/langchain_ollama/chat_models.py", line 1030, in _generate
    final_chunk = self._chat_stream_with_aggregation(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/langchain_ollama/chat_models.py", line 965, in _chat_stream_with_aggregation
    for chunk in self._iterate_over_stream(messages, stop, **kwargs):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/langchain_ollama/chat_models.py", line 1054, in _iterate_over_stream
    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/langchain_ollama/chat_models.py", line 952, in _create_chat_stream
    yield from self._client.chat(**chat_params)
  File "/usr/local/lib/python3.12/site-packages/ollama/_client.py", line 179, in inner
    raise ResponseError(e.response.text, e.response.status_code) from None
ollama._types.ResponseError: model 'llama3' not found (status code: 404)
During task with name 'explore' and id 'e00839ec-a082-6145-09d7-7e8f2e26b6fd'

